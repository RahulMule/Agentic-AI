# API Genius â€” Agentic AI for REST API Generation from Functional Specs

## ğŸš€ Overview

**API Genius** is a modular, multi-agent AI application that automatically generates **REST APIs** from **functional specification documents (FSDs)**.

Designed for backend developers, API Genius reads your requirements, extracts business logic, generates database schemas, and plans backend APIs â€” all using intelligent AI agents powered by **LlamaIndex**, **Streamlit**, and **Groq LLMs**.

## ğŸ¯ Key Highlights

* Streamlit-powered **interactive UI** for ease of use
* Uses **Groq-hosted LLaMA 3.1 models** (free with limits) for reliable responses
* Step-by-step **agent-based orchestration** using [LlamaIndex FunctionAgent + AgentWorkflow](https://docs.llamaindex.ai/en/stable/examples/agents/function_agents/)
* Modular and extensible pipeline for parsing specs, generating schema, APIs, OpenAPI docs, tests, and more
* Optional support for **local LLMs via Ollama**

## ğŸ§  Agent Architecture

| Agent Name                | Purpose                                                             |
| ------------------------- | ------------------------------------------------------------------- |
| `RequirementsParserAgent` | Parses FSDs and extracts requirements via RAG and tool calling only |
| `SchemaGeneratorAgent`    | Creates database schemas or Pydantic models                         |
| `CodeGeneratorAgent`      | *(Planned)* Generate FastAPI route logic                            |
| `OpenAPIGeneratorAgent`   | *(Planned)* Create OpenAPI v3 specs                                 |
| `TestCaseGeneratorAgent`  | *(Planned)* Build test cases based on constraints                   |

All agents are built using **FunctionAgent** and **tool calling**, with memory between stages.

## ğŸ“Š Tech Stack

| Tool / Library          | Role                                                              |
| ----------------------- | ----------------------------------------------------------------- |
| **LlamaIndex**          | RAG, FunctionAgent, indexing, context memory                      |
| **Groq LLaMA 3.1 API**  | Fast and accurate hosted LLM (free with usage caps)               |
| **Streamlit**           | Frontend to interactively run and view agent execution            |
| **Python**              | Core language                                                     |
| **Ollama (optional)**   | Local LLM runtime (Mistral or Code LLaMA) â€” fallback if preferred |
| **FastAPI** *(planned)* | Target backend framework to generate APIs                         |

## ğŸ’» System Requirements

* âœ… Works with **Groq** hosted LLMs â€” no GPU required
* âš™ï¸ Python 3.10+
* ğŸ’¼ Optional: 8GB+ GPU if running Ollama locally
* ğŸ“± Internet access needed for Groq model usage

## ğŸ› ï¸ Installation

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/apigenius.git
cd apigenius
```

### 2. Set up Python environment

```bash
python -m venv .venv
source .venv/bin/activate        # macOS/Linux
.venv\Scripts\activate           # Windows

pip install --upgrade pip
pip install -r requirements.txt
```

### 3. Configure your secrets

Create a `.env` file in the root:

```env
GROQ_API_KEY=your_groq_api_key_here
```

> You can get your key from [https://console.groq.com](https://console.groq.com)

## âºï¸ Running the App

### â–¶ï¸ Option 1: Using Streamlit UI

```bash
streamlit run app.py
```

* Upload or edit your FSD document
* Click "Run Agents"
* View live streaming output from each agent
* Final schema and requirements will be saved in `./data/req.txt`

### â–¶ï¸ Option 2: CLI Execution

```bash
python main.py
```

The agents will stream step-by-step reasoning and tool usage to the console. Output is saved in `./data/req.txt`.

## ğŸ“ Project Structure

```
apigenius/
â”œâ”€â”€ agents/                # Agent definitions
â”œâ”€â”€ config/                # Pydantic settings + secrets
â”œâ”€â”€ data/                  # Input/output files
â”œâ”€â”€ prompts/               # Prompt templates for agents
â”œâ”€â”€ app.py                 # Streamlit frontend
â”œâ”€â”€ main.py                # CLI entrypoint
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # This file
```

## â˜ï¸ Azure Web App Deployment (Optional)

To deploy on Azure:

1. Set up an Azure Web App for Python
2. Add your `GROQ_API_KEY` as an application setting in Azure Portal
3. Set startup command:

```bash
streamlit run app.py --server.port 8000
```

4. Push code via GitHub Actions or VSCode

## ğŸ” Environment & Secrets

Manage secrets using **Pydantic Settings** via `.env`:

```python
# config/secrets.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    groq_api_key: str

    class Config:
        env_file = ".env"
```

Use in agents:

```python
from config.secrets import Settings
settings = Settings()
```

## âš¡ Notes

* Groq LLaMA 3.1 models are **significantly faster** and more accurate than local Mistral in most cases
* Local models (Ollama) are still supported for offline fallback
* GPU recommended for local models: RTX 4060 or higher

## ğŸ”® Future Enhancements

* âœ… Streamlit frontend (added)
* ğŸ§  Code generation agent (planned)
* ğŸ“„ OpenAPI doc generation
* ğŸ§ª Automated test case generation
* â˜ï¸ Optional database scaffolding and FastAPI project export

## ğŸ“š References

* [Groq](https://console.groq.com)
* [LlamaIndex](https://docs.llamaindex.ai/)
* [Streamlit](https://streamlit.io/)
* [FastAPI](https://fastapi.tiangolo.com/)
* [Ollama](https://ollama.com/)

## ğŸ—ª License

MIT License Â© 2025

Feel free to â­ the repo and contribute!
